{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3db7a7d-f1d7-46d0-94c5-108649c77602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/oumayma/anaconda3/lib/python3.11/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3271c6af-fee4-420f-9d50-53041ba2fab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece version: 0.2.0\n",
      "Les résumés ont été enregistrés dans 'cluster_summaries_pegasus.json'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Désactiver les avertissements TensorFlow\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)  # Désactiver les avertissements Transformers\n",
    "\n",
    "import json\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Vérifier si SentencePiece est installé\n",
    "try:\n",
    "    import sentencepiece\n",
    "    print(f\"SentencePiece version: {sentencepiece.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"SentencePiece n'est pas installé. Veuillez l'installer avec 'pip install sentencepiece'.\")\n",
    "    exit(1)\n",
    "\n",
    "# Charger les données clusterisées\n",
    "def load_clustered_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Charger le modèle et le tokenizer PEGASUS\n",
    "def load_pegasus_model():\n",
    "    try:\n",
    "        # Charger le modèle pré-entraîné PEGASUS et le tokenizer\n",
    "        model_name = \"google/pegasus-xsum\"  # Modèle PEGASUS pour le résumé abstrait\n",
    "        tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du modèle PEGASUS : {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Fonction pour générer un résumé avec PEGASUS\n",
    "def generate_summary_pegasus(text, tokenizer, model, max_length=130, min_length=30):\n",
    "    try:\n",
    "        # Tokeniser le texte d'entrée\n",
    "        inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        # Générer le résumé\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=min_length, num_beams=5, early_stopping=True)\n",
    "        \n",
    "        # Décoder le résumé en texte\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la génération du résumé avec PEGASUS : {e}\")\n",
    "        return \"Résumé non disponible\"\n",
    "\n",
    "# Chemin vers le fichier clusterisé\n",
    "clustered_file_path = 'clustered_articles2.json'\n",
    "clustered_data = load_clustered_data(clustered_file_path)\n",
    "\n",
    "# Regrouper les articles par cluster\n",
    "clusters = {}\n",
    "for article in clustered_data:\n",
    "    cluster_id = article['cluster']\n",
    "    if cluster_id not in clusters:\n",
    "        clusters[cluster_id] = []\n",
    "    clusters[cluster_id].append(article['content'])\n",
    "\n",
    "# Charger le modèle et le tokenizer PEGASUS\n",
    "tokenizer, model = load_pegasus_model()\n",
    "if tokenizer is None or model is None:\n",
    "    print(\"Impossible de charger le modèle PEGASUS. Vérifiez les erreurs ci-dessus.\")\n",
    "    exit(1)\n",
    "\n",
    "# Générer un résumé pour chaque cluster\n",
    "cluster_summaries = {}\n",
    "for cluster_id, contents in clusters.items():\n",
    "    # Concaténer le contenu de tous les articles du cluster\n",
    "    combined_content = \" \".join(contents)\n",
    "    \n",
    "    # Limiter la longueur du texte pour respecter les limites du modèle\n",
    "    max_input_length = 1024  # Limite de tokens pour PEGASUS\n",
    "    if len(combined_content) > max_input_length:\n",
    "        combined_content = combined_content[:max_input_length]\n",
    "    \n",
    "    # Générer un résumé avec PEGASUS\n",
    "    summary = generate_summary_pegasus(combined_content, tokenizer, model, max_length=130, min_length=30)\n",
    "    cluster_summaries[cluster_id] = summary\n",
    "\n",
    "# Sauvegarder les résumés dans un fichier JSON\n",
    "output_file_path = 'cluster_summaries_pegasus.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(cluster_summaries, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Les résumés ont été enregistrés dans '{output_file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2a8cc-c1ee-41a7-a172-ae21d5c89f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
